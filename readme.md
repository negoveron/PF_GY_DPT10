# <h1 align=center> **Proyecto Grupal - HENRY DATA ANALYST** </h1>

## <h1 align=center>**`Yelp - Google`**</h1>

<p align="center">
<img src="Imagenes/GastroSphere_portada.jpg"  
height=450>
</p>


<details>
  <summary><h2> Primer Sprint</h2> </summary>

## 1.- Objetivos:  
‚óè An√°lisis de Sentimientos:   
Aplicar t√©cnicas de NLP (Natural Language Processing) para
clasificar rese√±as de Yelp y Google Maps como positivas, negativas o neutras. Adem√°s nos enfocaremos en aspectos espec√≠ficos como:  
‚óè Comentarios relacionados con el servicio (amabilidad del personal, tiempo de
espera, atenci√≥n al cliente).  
‚óè Condiciones del ambiente (limpieza, ruido, confort).  
‚óè Factores espec√≠ficos de la cocina (calidad de la comida, temperatura, presentaci√≥n).  
‚óè Predicci√≥n de Tendencias del Mercado: Implementar modelos de machine learning para predecir tendencias en el mercado de restaurantes, basados en datos hist√≥ricos y actuales,con el objetivo de identificar tipos de restaurantes con mayor probabilidad de crecimientoo declive.  
‚óè Recomendaci√≥n de Ubicaciones para Nuevos Restaurantes: Realizar un an√°lisis
geo-espacial a fin de identificar regiones con alta concentraci√≥n de restaurantes (conocidas como polos gastron√≥micos). Este an√°lisis debe segmentar la informaci√≥n seg√∫n la cantidad de estrellas y el tipo de restaurante, proporcionando una visi√≥n clara y estrat√©gica para abrir nuevos restaurantes.  
‚óè Sistema de Recomendaci√≥n de Restaurantes: desarrollar un sistema de recomendaci√≥n de restaurantes que provea a los usuarios sugerencias personalizadas basadas en sus preferencias individuales y experiencias previas. Este sistema utilizar√° t√©cnicas avanzadas de machine learning y an√°lisis de datos para ofrecer recomendaciones que no solo se
adapten a los gustos y h√°bitos de los usuarios, sino que tambi√©n tengan en cuentavfactores contextuales, como la hora del d√≠a, la ubicaci√≥n actual y las tendencias recientes en las rese√±as.  
## 2.- Alcance del Proyecto
An√°lisis del Mercado y Recomendaci√≥n de Restaurantes
1. Objetivos y Metas del Proyecto:  
‚óè An√°lisis exhaustivo del mercado: Realizar un an√°lisis detallado del mercado
estadounidense, enfocado en restaurantes, utilizando datos de Yelp y Google Maps.
‚óè Modelo de Machine Learning: Implementar un modelo para el an√°lisis de
sentimientos y predicci√≥n de tendencias del mercado de restaurantes.
‚óè Sistema de Recomendaci√≥n: Desarrollar un sistema de recomendaci√≥n de
restaurantes personalizado a los usuarios finales.
2. Entregables:  
‚óè DataWarehouse en Google Cloud:
- Configuraci√≥n de Google Cloud para el almacenamiento y gesti√≥n de datos
de Yelp y Google Maps.
- Implementaci√≥n de procesos ETL (Extracci√≥n, Transformaci√≥n y Carga) para
la integraci√≥n de datos.
- Realizaci√≥n de un An√°lisis Exploratorio de Datos (EDA) completo para
descubrir patrones y tendencias.
‚óè .An√°lisis de Sentimientos:
Informes detallados que clasifiquen las rese√±as como positivas, negativas o
neutras.
‚óè Modelos Predictivos:
Predicciones sobre el crecimiento o declive de restaurantes basadas en
datos hist√≥ricos y actuales.
‚óè Sistema de Recomendaci√≥n:
Prototipo funcional que ofrezca sugerencias personalizadas de restaurantes.
‚óè An√°lisis Espacial:
Recomendaciones de ubicaciones √≥ptimas para nuevos restaurantes,
basadas en un an√°lisis espacial y demogr√°fico.
‚óè Dashboard Interactivo:
Visualizaci√≥n de los resultados del an√°lisis de sentimientos, predicciones de
tendencias, recomendaciones de ubicaciones.
3. Tareas y Actividades:
‚óè Recolecci√≥n y Limpieza de Datos:
Obtenci√≥n y limpieza de datos desde Google Maps y Yelp.
‚óè Dise√±o y Desarrollo del DataWarehouse:
Dise√±o e implementaci√≥n del DataWarehouse en Google Cloud.
‚óè Implementaci√≥n de Procesos ETL:
Desarrollo de pipelines para la extracci√≥n, transformaci√≥n y carga de datos.
‚óè An√°lisis Exploratorio de Datos (EDA):
Realizaci√≥n de EDA para analizar y comprender los datos.
‚óè Desarrollo y Validaci√≥n de Modelos de Machine Learning:
Entrenamiento y evaluaci√≥n de modelos para an√°lisis de sentimientos y
predicci√≥n de tendencias.
‚óè Creaci√≥n de Dashboards Interactivos en Power BI:
Desarrollo de dashboards para la visualizaci√≥n de datos y resultados.
‚óè Implementaci√≥n de la Metodolog√≠a Scrum:
Gesti√≥n del proyecto utilizando la metodolog√≠a Scrum con sprints y
reuniones diarias.
‚óè Desarrollo de Pipelines de Datos:
Dise√±o y desarrollo de pipelines para la automatizaci√≥n de procesos de
datos.
‚óè Documentaci√≥n:
Elaboraci√≥n de documentaci√≥n t√©cnica.
4. Exclusiones del Proyecto:
‚óè No se realizar√° una implementaci√≥n f√≠sica de nuevos restaurantes.
‚óè No se incluir√°n datos fuera del mercado estadounidense.
‚óè No se cubrir√°n otros rubros de negocio distintos a los restaurantes.
Restricciones:
‚óè Tiempo: El proyecto debe completarse en un plazo de 6 semanas.
‚óè Presupuesto: Limitado a la infraestructura de Google Cloud y herramientas de desarrollo
(ClickUp, Python).
‚óè Recursos: Equipo limitado con roles definidos (Scrum Master, Product Owner, Equipo de
Desarrollo).
## 3.- KPIs
‚óè Aumentar en 5% la raz√≥n de rese√±as positivas de un trimestre con respecto al anterior
para un restaurante particular. La raz√≥n de rese√±as positivas se calcula como: rese√±as
positivas (por trimestre) / total de rese√±as (por trimestre).
‚óè Aumentar trimestralmente en 1 (uno) la cantidad de categor√≠as de reviews donde la
relaci√≥n de las positivas sobre sea mayor a 1. Las categor√≠as de reviews salen de la
clasificaci√≥n del an√°lisis de sentimientos y pueden ser positivas, negativas o neutrales.
‚óè Bajar trimestralmente en 10% la calificaci√≥n negativa (2 o menos estrellas).
## 4.- EDA
La realizaci√≥n de un ETL y EDA preliminar permiten tener un primer acercamiento a la
consistencia de los datos, identificar qu√© datos est√°n disponibles y aumentar el entendimiento de
los mismos.
Para la Demo # 1 se realizaron ETL y EDA preliminares los que arrojaron las primeras
impresiones:
‚óè Las reviews son en su mayor√≠a positivas
‚óè La mayor√≠a de las calificaciones son de 3 o m√°s estrellas, 2 estrellas es lo menos
frecuente.
‚óè La categor√≠a useful es la m√°s frecuente a la hora de clasificar una review.
‚óè M√°s del 50% de los usuarios hizo 6 o menos reviews.
‚óè Los rubros m√°s frecuentes son restaurant y food.
‚óè La mayor√≠a de los negocios cuenta con menos de 20 reviews, con la media en 10 reviews.
‚óè Los restaurantes con calificaciones m√°s bajas (alrededor de 1 y 2 ) tienden a tener muy
pocas rese√±as.
## 5.- Metodolog√≠a
Metodolog√≠a de Trabajo: Scrum dentro del Marco √Ågil
Enfoque √Ågil
El proyecto se gestionar√° utilizando el marco √Ågil (Agile), que permite un desarrollo flexible,
adaptable y basado en la retroalimentaci√≥n continua. Este enfoque prioriza la entrega
incremental de valor al cliente y responde eficazmente a los cambios y necesidades del entorno.
Dentro de este marco, implementaremos Scrum, una metodolog√≠a √°gil ampliamente utilizada para
gestionar proyectos de software y desarrollo de productos. Scrum nos permitir√° dividir el trabajo
en ciclos cortos, llamados sprints, facilitando la evaluaci√≥n continua y la mejora del producto a
trav√©s de reuniones peri√≥dicas y retroalimentaci√≥n constante.
Etapas de la Metodolog√≠a Scrum
1. Product Backlog
Se recopilar√°n todas las historias de usuario, requisitos y tareas necesarias para el
desarrollo del proyecto. Estas historias representan las funcionalidades clave y se
priorizar√°n seg√∫n su impacto en los objetivos del sistema de alertas y predicci√≥n.
2. Sprint Planning
Al inicio de cada sprint (2 semanas), se llevar√° a cabo una reuni√≥n de planificaci√≥n del
sprint donde el equipo seleccionar√° las historias de usuario m√°s relevantes del backlog.
Estas se dividir√°n en tareas m√°s peque√±as y manejables, asegurando que el equipo tenga
una carga de trabajo realista y alcanzable.
3. Sprints
Durante cada sprint, el equipo trabajar√° en las tareas seleccionadas, siguiendo los
principios de desarrollo iterativo e incremental. El progreso se monitorear√° mediante
ClickUp, permitiendo transparencia en el avance y las prioridades.
Para cada tarea decidimos entre el grupo desarrollador que haya un encargado que haga
la revisi√≥n t√©cnica, divida tareas y tambi√©n escriba c√≥digo.
4. Daily Scrum (Reuniones Diarias)
Se realizar√°n reuniones cortas diarias (~60 minutos) para que cada miembro del equipo
reporte lo siguiente:
‚óã Qu√© se hizo el d√≠a anterior.
‚óã Qu√© se va a hacer hoy.
‚óã Si hay alg√∫n obst√°culo o problema.
Van a ser 3 veces por semana (Lunes, Mi√©rcoles y Viernes) con el Scrum Master y Martes y
Jueves solo el grupo desarrollador e implementando mas si es necesario durante los fines
de semana
Esto fomenta la comunicaci√≥n, identifica problemas r√°pidamente y asegura que todos
est√©n alineados con los objetivos del sprint.
5. Sprint Review (Demo)
Al final de cada sprint, se presentar√° el producto o incremento desarrollado a los
interesados clave. El objetivo es recibir retroalimentaci√≥n directa para adaptar el desarrollo
a las expectativas del cliente y hacer ajustes para futuros sprints.
6. Sprint Retrospectiva
Despu√©s de la revisi√≥n del sprint, el equipo participar√° en una retrospectiva donde se
reflexionar√° sobre:
‚óã Qu√© se hizo bien.
‚óã Qu√© se puede mejorar.
‚óã Acciones para optimizar el proceso en el siguiente sprint.
Esto asegura una mejora continua no solo en el producto, sino tambi√©n en la metodolog√≠a
y colaboraci√≥n del equipo.
Roles Clave en Scrum
1. Product Owner : Pia Ruiz Jaimes
Es el responsable de definir las prioridades del backlog, asegurando que el equipo trabaje
en las tareas m√°s importantes para el cliente. Adem√°s, se encarga de asegurar que el
producto cumpla con los objetivos estrat√©gicos.
2. Scrum Master : Alfredo Trujillo
El Scrum Master facilita el proceso de Scrum, eliminando impedimentos y ayudando al
equipo a seguir las pr√°cticas de Scrum correctamente. Tambi√©n protege al equipo de
interrupciones externas y facilita la colaboraci√≥n.
3. Development Team : Guillermo Veron, Joaquin Costamagna, Jose Cruz, Maria Jose
Atencio, Fernando Garcias Corts
Un equipo auto organizado de desarrolladores, analistas de datos y otros roles t√©cnicos
que se encargan de llevar a cabo las tareas seleccionadas del sprint backlog. El equipo es
responsable de entregar incrementos de valor al final de cada sprint.
## 6.- Stack Tecnol√≥gico
#### Backend:  
‚óè Lenguaje Principal: Python  
‚óè Framework: FastAPI (TBD)  
‚óè Base de Datos: PostgreSQL (TBD)  
‚óè Algoritmos y Librer√≠as de Recomendaci√≥n:  
‚óã TextBlob: Para el an√°lisis de sentimiento de las reviews  
‚óã Scikit-learn: Para el c√°lculo de la similitud de coseno.(TBD)  
‚óã HDBSCAN: Para la clusterizaci√≥n de restaurantes.  
‚óã Pandas: Para la manipulaci√≥n de los datos.  
‚óã Numpy: Para las operaciones num√©ricas.  
‚óè API REST: Endpoints creados con FastAPI para la comunicaci√≥n entre el frontend y el backend. (TBD)  
#### Frontend:  
‚óè Lenguaje Principal: Python  
‚óè Framework: React (TBD)  
‚óè Estilizaci√≥n: CSS/Bootstrap/Tailwind (TBD)  
Infraestructura y Deploy:  
‚óè Proveedor de Hosting: Render (TBD)  
‚óè Contenedores: Google Cloud  
‚óè Control de Versiones: Git y GitHub (https://github.com/negoveron/PF_GY_DPT10)  
#### An√°lisis de Datos y Visualizaci√≥n:  
‚óè Power BI: Utilizado para generar reportes interactivos y visualizaciones de los datos del sistema de recomendaci√≥n.  
‚óã Visualizaciones: Se presentan gr√°ficos que muestran las m√©tricas de precisi√≥n ,como la cantidad de usuarios que coincidieron en gustos, la precisi√≥n de las recomendaciones, y las restaurants m√°s recomendados.  
‚óã Conexi√≥n a Datos: Power BI se conectar√° a la base de datos Google Cloud para
extraer los datos necesarios y realizar el an√°lisis visual.  
#### Herramientas Adicionales:  
‚óè Editor de C√≥digo: Visual Studio Code  
‚óè Herramientas de Colaboraci√≥n:  
‚óã Clickup (https://app.clickup.com/9011445165/v/li/901105723447) / Slack / Google Meet  

<p align="center">
<img src="Imagenes/stack.png"  
height=450>
</p>


</details>




<details>
  <summary><h2>Segundo Sprint</h2></summary>

### ETL Completo

- [Carpeta Notebooks ETL](https://github.com/negoveron/PF_GY_DPT10/tree/0af87f6a9a777b1032d4f1f09d5176896dc36245/Demo_2/ETL2)

### Estructura de Datos Implementada (DW)
## Ciclo de vida del dato 

<p align="center">
<img src="Imagenes/ciclo_de_vida.png"  
height=450>
</p>

El ciclo de vida de nuestros datos comienza con la recolecci√≥n de informaci√≥n desde diversas fuentes externas, incluyendo bases de datos relacionales, archivos de tipo JSON y Parquets de terceros. Estas fuentes externas contienen datos en m√∫ltiples formatos y estructuras, lo que presenta el primer desaf√≠o: asegurar que la informaci√≥n pueda ser unificada y estandarizada para su procesamiento y an√°lisis.

En la siguiente etapa, los datos pasan a trav√©s de un pipeline ETL (Extracci√≥n, Transformaci√≥n y Carga). En la fase de extracci√≥n, los datos son capturados desde sus fuentes originales y transferidos al entorno de procesamiento para realizar una `validacion de datos` para realizar un primer tratamiento. Luego, en la transformaci√≥n, aplicamos reglas y procedimientos para limpiar, estandarizar y enriquecer los datos, eliminando duplicados, gestionando valores nulos y asegurando que cumplan con los requisitos de calidad y estructura para su uso posterior. Finalmente, en la fase de carga, los datos transformados son transferidos a un entorno intermedio o directamente al almac√©n de datos (Data Warehouse) para su almacenamiento y an√°lisis.

Una vez que los datos han pasado por el pipeline ETL y se encuentran en un formato consistente, ingresan a un Data Warehouse a trav√©s de un segundo pipeline dise√±ado para gestionar su integraci√≥n en un almac√©n de datos centralizado. Este Data Warehouse organiza los datos en estructuras optimizadas para la consulta, facilitando el acceso y la recuperaci√≥n r√°pida de informaci√≥n para an√°lisis profundos y generaci√≥n de reportes. Este almacenamiento permite gestionar grandes vol√∫menes de datos hist√≥ricos, ofreciendo una visi√≥n completa de la informaci√≥n consolidada de la organizaci√≥n.

Finalmente, los datos almacenados en el Data Warehouse est√°n listos para ser consumidos por herramientas de an√°lisis como ser Power BI. En esta √∫ltima etapa, Power BI permite a los usuarios crear reportes interactivos y dashboards visuales que facilitan la exploraci√≥n y an√°lisis de la informaci√≥n. Los reportes generados en Power BI proporcionan m√©tricas y tendencias clave, posibilitando una toma de decisiones informada y estrat√©gica. A trav√©s de estos dashboards, los usuarios pueden acceder a una representaci√≥n visual de los datos, realizar an√°lisis en tiempo real y compartir insights de manera √°gil y eficiente.

Este proceso detallado compuesto por ambos pipelines conforman la `automatizacion` del ciclo de vida de los datos, que va desde la extracci√≥n y transformaci√≥n inicial hasta su an√°lisis y visualizaci√≥n final, asegura que los datos est√©n alineados con las necesidades anal√≠ticas de la organizaci√≥n y facilita la toma de decisiones fundamentadas en informaci√≥n confiable y actualizada.

### 1 _ Pipeline ETL Automatizado:

Se ha desarrollado un pipeline de datos, un proceso estructurado que nos permiti√≥ automatizar la captura, transformaci√≥n y carga de informaci√≥n de diferentes fuentes hacia un entorno de an√°lisis. Este pipeline cumple un rol clave en la gesti√≥n de datos moderna, ya que asegura la integraci√≥n y disponibilidad de datos actualizados y fiables para la toma de decisiones y an√°lisis de negocio.

#### Objetivos
- El principal objetivo del pipeline de datos es procesar informaci√≥n de manera eficiente, de tal forma que pueda ser transformada y cargada en un repositorio para su an√°lisis. Algunos objetivos espec√≠ficos incluyen:

- - Automatizar la extracci√≥n de datos desde fuentes heterog√©neas.
Estandarizar y limpiar los datos para su an√°lisis.
Coordinar las tareas de manera secuencial y con manejo de errores.
Generar un flujo de datos confiable y escalable.
Metodolog√≠a y Etapas del Pipeline
Para lograr estos objetivos, dividimos el pipeline en las siguientes etapas:

- Extracci√≥n de Datos: La primera etapa consiste en la extracci√≥n de datos desde fuentes de origen, que pueden incluir bases de datos relacionales, archivos JSON o APIs de terceros. Para asegurar la conectividad y disponibilidad de datos, empleamos herramientas de integraci√≥n y realizamos un monitoreo continuo de las fuentes de datos.

- Transformaci√≥n de Datos: En esta fase, se realizan las transformaciones necesarias para asegurar la calidad y estandarizaci√≥n de los datos. Esto incluye tareas de limpieza, normalizaci√≥n y eliminaci√≥n de duplicados, as√≠ como tambi√©n cualquier preprocesamiento espec√≠fico para adaptar los datos a los requisitos del an√°lisis. Utilizamos herramientas como pandas para manejar grandes vol√∫menes de datos de forma eficiente.

- Carga de Datos: Una vez que los datos han sido procesados y se encuentran en un formato adecuado, los cargamos en el datawarehouse de GoogleCloud donde se correra un pipelineDW para luego cargarlo a BigQuery

- Orquestaci√≥n y Automatizaci√≥n: Para gestionar la secuencia de las tareas y sus dependencias, hemos utilizado un orquestador de datos. Este orquestador nos permiti√≥ automatizar y programar la ejecuci√≥n del pipeline, monitorear el estado de cada etapa y asegurar que los errores sean manejados adecuadamente. Para esto, hemos utilizado AirFlow de la suite de GoogleCLoud, que facilita la programaci√≥n y ejecuci√≥n de flujos de trabajo complejos.

- Monitoreo y Gesti√≥n de Errores: La √∫ltima etapa en el pipeline implica un monitoreo activo de las tareas en ejecuci√≥n y el manejo de errores en tiempo real. Implementamos registros de logs y alertas para asegurar la detecci√≥n temprana de fallos, de manera que las tareas puedan ser reintentadas autom√°ticamente o escaladas para su resoluci√≥n.

#### Resultados
El pipeline desarrollado logr√≥ reducir el tiempo manual de procesamiento de datos y mejorar la calidad de la informaci√≥n analizada. Gracias a la automatizaci√≥n de las tareas de extracci√≥n, transformaci√≥n y carga, el equipo de an√°lisis ahora puede acceder a datos consistentes y en tiempo real. Esto ha permitido un an√°lisis m√°s √°gil y ha facilitado la toma de decisiones basada en datos actualizados.

#### Desaf√≠os y Soluciones
Durante el desarrollo del pipeline, enfrentamos varios desaf√≠os, tales como:

- - Integraci√≥n de fuentes heterog√©neas: Resolver incompatibilidades entre los distintos formatos de datos y asegurar la conectividad constante.
Escalabilidad: Optimizar el rendimiento del pipeline para manejar vol√∫menes crecientes de datos sin afectar la velocidad o calidad de los procesos.
- - Manejo de Errores: Implementar un sistema robusto de gesti√≥n de errores para asegurar que el pipeline pueda recuperarse de fallos y continuar con la ejecuci√≥n del flujo de datos.
- - Para resolver estos desaf√≠os, implementamos una estrategia de reintentos autom√°ticos, procesamiento en paralelo para ciertas tareas, y monitoreo continuo del estado del pipeline.

#### Conclusi√≥n:  
El desarrollo de este pipeline de datos representa una soluci√≥n integral para gestionar, transformar y analizar datos de manera eficiente. Esta implementaci√≥n de automatizaci√≥n y orquestaci√≥n de datos permite a la organizaci√≥n beneficiarse de un flujo de trabajo de datos confiable, escalable y alineado con las necesidades de an√°lisis. A futuro, planeamos mejorar el pipeline mediante la incorporaci√≥n de capacidades de escalabilidad en la nube y de machine learning para optimizar a√∫n m√°s el procesamiento de los datos y generar informaci√≥n de valor en tiempo real.

### 2_ Pipeline para alimentar el DW

Este pipeline, implementado como una funci√≥n en **Cloud Run**, est√° dise√±ado para automatizar la carga de datos de rese√±as provenientes de archivos CSV en Google Cloud Storage hacia BigQuery. La funci√≥n `function-carga-wh` valida los archivos entrantes, extrae los datos, los transforma y los carga en BigQuery para su posterior an√°lisis. Este proceso permite manejar eficientemente los datos de rese√±as de Yelp y Google Maps, haciendo posible el an√°lisis de la opini√≥n de los usuarios y facilitando la creaci√≥n de modelos de machine learning para recomendaciones y predicciones de mercado.

#### Componentes principales

##### Funciones y archivos
- **validate_data**: Valida que el archivo contenga las columnas requeridas.
- **load_to_bigquery**: Carga los datos en BigQuery, a√±adi√©ndolos a la tabla de destino especificada en las variables de entorno.
- **read_csv**: Lee el archivo CSV desde Google Cloud Storage.
- **name_validation**: Valida el nombre y el tipo de archivo.
- **gcs_to_bigquery**: Funci√≥n principal que orquesta el flujo de trabajo.

#### Arquitectura del pipeline

1. **Disparador de Eventarc**: Configurado para escuchar eventos de finalizaci√≥n (`google.cloud.storage.object.v1.finalized`) en el bucket `test-pfgydpt10-bucket` de Google Cloud Storage. Cuando se sube un nuevo archivo CSV, se activa la funci√≥n `function-carga-wh`.
2. **Validaci√≥n del archivo**: La funci√≥n `name_validation` verifica que el archivo tenga el prefijo `tip` y sea de tipo `csv` antes de continuar con el procesamiento.
3. **Lectura y validaci√≥n de datos**: Si la validaci√≥n es exitosa, `read_csv` lee el archivo y `validate_data` revisa que contenga las columnas necesarias (`user_id`, `business_id`, `text`, `date`).
4. **Carga a BigQuery**: Una vez validado, el archivo se carga en BigQuery utilizando `load_to_bigquery`. Si el proceso falla, el error se registra en Cloud Logging.
5. **Logging**: La funci√≥n utiliza Cloud Logging para registrar errores y mensajes de seguimiento, permitiendo identificar problemas en el pipeline.

#### Variables de entorno

Estas variables son necesarias para configurar el entorno y deben ser configuradas en Cloud Run:

- `BQ_PROJECT_ID`: ID del proyecto de BigQuery.
- `BQ_DATASET_ID`: ID del dataset de BigQuery donde se cargan los datos.
- `BQ_TABLE_ID`: ID de la tabla de BigQuery.
- `LOG_EXECUTION_ID`: Flag opcional para habilitar o deshabilitar el logeo de ejecuci√≥n.

### 3_ Data Warehouse

Se ha decidido por la opci√≥n de una estructura de DataWarehouse por las siguientes razones:

1. **Estructura de los Datos**:

   - Los datos que se manejan son estructuradosy organizados en tablas que han pasado por un proceso de limpieza y transformaci√≥n (ETL). Esto permite optimizar el acceso a los datos y realizar consultas r√°pidas y complejas.
2. **Prop√≥sito de los Datos**:

   - Una de las principales prop√≥sitos es el an√°lisis de datos hist√≥ricos, con la posibilidad de una carga incremental. Estos datos estan bien definidos y depurados lo cual los hace mas compatible con un Data Warehouse
3. **Velocidad de Consulta y Rendimiento**:

   - Los *data warehouses* est√°n dise√±ados para optimizar consultas r√°pidas en datos estructurados. Pueden entregar resultados de consultas en cuesti√≥n de segundos o minutos, ideal para aplicaciones de business intelligence.
4. **Costos y Mantenimiento**:

   - Los *data warehouses*, al implicar procesos de transformaci√≥n de datos y optimizaci√≥n para consultas, pueden tener costos de almacenamiento y procesamiento superiores. Sin embargo, estos costos se justifican en entornos donde la rapidez y precisi√≥n en las consultas es fundamental para la toma de decisiones.

Finalemente ponderando la facilidad de uso, la capcidad de an√°lisis r√°pidos y confiables de grandes cantidades de datos estructurados, a un costo inicial de $0 se opt√≥ por usar Cloud Storage Platform.

### Dise√±o ER

<p align="center">
<img src="Imagenes/Yelp_ER.png"  
height=550>
</p>

### Documentacion

- [Diccionario de datos](https://docs.google.com/document/d/1ASLMGAgrviicATaP1UJlflpmBCXtuSTHQGWdQMN6_2I/edit)

### Analisis de datos de muestra

El an√°lisis exploratorio de datos (EDA) realizado sobre una muestra del conjunto de datos de negocios de Yelp y Google Maps se enfoca en identificar patrones y tendencias relacionadas con la ubicaci√≥n, estado, y calificaci√≥n de negocios en el mercado estadounidense. Este an√°lisis proporciona una visi√≥n preliminar del estado de los negocios, sus calificaciones promedio y su distribuci√≥n geogr√°fica, lo que puede ser √∫til para estrategias de marketing, optimizaci√≥n de localizaci√≥n y evaluaci√≥n de la satisfacci√≥n del cliente. Dicho EDA se enfoc√≥ en: 


1. **Estado de Apertura de los Negocios**:
   La mayor√≠a de los negocios en el dataset est√°n activos. Un an√°lisis detallado muestra que alrededor de un 80% de los negocios est√°n abiertos, mientras que aproximadamente el 20% est√°n cerrados. Esta proporci√≥n puede ser √∫til para evaluar la estabilidad de negocios en distintas ubicaciones.

2. **Distribuci√≥n de Negocios por Estado**:
   Los estados de Pensilvania (PA) y Florida (FL) cuentan con la mayor cantidad de negocios en el dataset, seguidos de Tennessee (TN), Indiana (IN), y Misuri (MO). La alta densidad en ciertos estados puede reflejar una mayor competencia y saturaci√≥n en estos mercados, lo cual es relevante para futuras estrategias de expansi√≥n.

3. **Distribuci√≥n de Calificaciones**:
   Las calificaciones de los negocios muestran una tendencia positiva, con la mayor√≠a de los negocios ubicados entre 3 y 5 estrellas. La categor√≠a m√°s frecuente es la de 4 estrellas, lo que sugiere que, en general, los clientes est√°n satisfechos con la calidad de los servicios. Las calificaciones m√°s bajas son menos comunes, lo que indica un sesgo hacia valoraciones positivas.

<br><br>
<p align="center">
<img src="Imagenes/Calificaci√≥n_de_negocios.png"  
height=450>
</p>
<br><br>

5. **An√°lisis de Rese√±as por Calificaci√≥n**:
   Los negocios con calificaciones altas (4 y 5 estrellas) tienden a tener una mayor cantidad de rese√±as, indicando una correlaci√≥n entre la popularidad de los negocios y su calificaci√≥n. Este hallazgo es valioso para identificar negocios destacados que atraen mayor atenci√≥n y fidelizaci√≥n de clientes.

6. **An√°lisis de Ciudades**:
   Filadelfia, Tucson, y Tampa lideran en cantidad de negocios dentro del dataset. Esta concentraci√≥n puede ser indicativa de √°reas con una alta actividad comercial, especialmente en sectores de turismo y restauraci√≥n. Tambi√©n se identificaron m√°s de 1,400 ciudades √∫nicas, lo que muestra una buena representatividad geogr√°fica en los datos.

7. **Matriz de Correlaci√≥n**:
   La matriz de correlaci√≥n entre variables num√©ricas como `latitude`, `longitude`, `stars`, `review_count`, y `is_open` muestra correlaciones bajas, indicando que las variables son en su mayor√≠a independientes entre s√≠. Este an√°lisis sugiere que no existen relaciones lineales fuertes, lo cual orienta la necesidad de m√©todos avanzados para un an√°lisis m√°s profundo.

8. **Nube de Palabras en Nombres de Negocios**:
   La nube de palabras generada muestra que t√©rminos como "Cafe," "Restaurant," "Bar," "Pizza," y "Grill" son los m√°s frecuentes en los nombres de negocios, destacando la prevalencia de la industria de alimentos y bebidas en el dataset.

<br><br>
<p align="center">
<img src="Imagenes/Nube_de_palabras.png"  
height=450>
</p>
<br><br>


- [Link al EDA](https://github.com/negoveron/PF_GY_DPT10/blob/main/EDA/EDA.ipynb)

### MVP/Proof of concept de producto ML

<br>

<p align="center">
<img src="Imagenes/Dashboard_1.jpeg"  
height=450>
</p>


<br><br><br>


<p align="center">
<img src="Imagenes/Dashboard_2.jpeg"  
height=450>
</p>

- [Link al Dashboard](https://docs.google.com/document/d/1ASLMGAgrviicATaP1UJlflpmBCXtuSTHQGWdQMN6_2I/edit)

</details>  


<details>
  <summary><h2> Tercer Sprint </h2></summary>

  Coming soon..

</details>

## Autores! üëã

|   <h3 align=center>Foto</h3> | [![linkedin](https://img.shields.io/badge/linkedin-0A66C2?style=for-the-badge&logo=linkedin&logoColor=white)](https://www.linkedin.com/) |
|--------|-------------|
| <img src="Imagenes/jc2_perfil.png" width="75"/> | [Linkedin Jos√©](https://www.linkedin.com/in/jose-yesid-cruz-pinto/) |
| <img src="Imagenes/jc_perfil.png" width="75"/> | [Linkedin Joaquin](https://www.linkedin.com/in/joaquin-costamagna-028654215/) |
| <img src="Imagenes/mj_perfil.png" width="75"/> | [Linkedin Maria Jose](https://www.linkedin.com/in/maria-jose-atencio-96a8761aa/) |
| <img src="Imagenes/fmgc_perfil.png" width="75"/> | [Linkedin Fernando](https://www.linkedin.com/in/fernando-garcias-corts-326a4027) |
| <img src="Imagenes/gv_perfil.png" width="75"/> | [Linkedin Guillermo](https://www.linkedin.com/in/guillermo-andres-veron/) |

